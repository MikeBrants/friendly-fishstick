# SAM DELIVERABLES - Test Analysis & Gap Assessment
**Date**: 2026-01-26
**Mission**: Validation protocol and tests complementary to Jordan's WFE DUAL + PBO integration
**Status**: COMPLETE

---

## S1: PBO Tests Review

### Existing Tests
**File**: `c:\Users\Arthur\friendly-fishstick\tests\validation\test_guard008.py`
**Test Count**: 8 tests
**Status**: ‚úÖ ALL PASS (assumed based on Jordan's report of 12/12 tests passing)

### Test Coverage Analysis

#### ‚úÖ WELL COVERED
1. **Function existence**: `test_guard_pbo_exists()` - Verifies import and callable
2. **Return format**: `test_guard_pbo_return_format()` - Validates dict structure and keys
3. **PBO range**: `test_pbo_probability_range()` - Ensures PBO in [0, 1]
4. **Threshold logic**: `test_pbo_threshold_logic()` - Tests pass/fail based on 0.30 threshold
5. **Parameter validation**: `test_pbo_n_splits_validation()` - Validates n_splits even and >= 2
6. **Interpretation**: `test_pbo_interpretation()` - Checks interpretation string generation
7. **Perfect strategy**: `test_guard_pbo_with_perfect_strategy()` - Tests always-win scenario
8. **Random strategy**: `test_guard_pbo_with_random_strategy()` - Tests random returns

### Coverage Gaps Identified

#### üî¥ CRITICAL GAPS (High Priority)

**GAP-1: Empty Returns Matrix**
```python
def test_pbo_empty_returns_matrix():
    """Test PBO with empty returns matrix (should raise or handle gracefully)."""
    empty_returns = np.array([]).reshape(0, 0)

    # EXPECTED: Should raise ValueError with clear message
    with pytest.raises(ValueError, match="returns_matrix cannot be empty"):
        guard_pbo(empty_returns, threshold=0.30, n_splits=4)
```
**Why Critical**: Production pipeline may pass empty arrays if optimization fails silently.

---

**GAP-2: n_splits = 0 or negative**
```python
def test_pbo_zero_splits():
    """Test PBO with n_splits=0 (should raise ValueError)."""
    returns = np.random.randn(10, 100) * 0.01

    with pytest.raises(ValueError, match="n_splits must be at least 2"):
        probability_of_backtest_overfitting(returns, n_splits=0)

def test_pbo_negative_splits():
    """Test PBO with negative n_splits (should raise ValueError)."""
    returns = np.random.randn(10, 100) * 0.01

    with pytest.raises(ValueError, match="n_splits must be at least 2"):
        probability_of_backtest_overfitting(returns, n_splits=-4)
```
**Why Critical**: Edge case that would cause cryptic numpy indexing errors.

---

**GAP-3: Insufficient Periods for Splits**
```python
def test_pbo_insufficient_periods():
    """Test PBO when n_periods < n_splits (should raise ValueError)."""
    returns = np.random.randn(10, 8) * 0.01  # Only 8 periods, but need 16 for n_splits=16

    with pytest.raises(ValueError, match="Not enough periods"):
        probability_of_backtest_overfitting(returns, n_splits=16)
```
**Why Critical**: Assets with short history (e.g., new listings) would crash pipeline.

---

**GAP-4: All NaN Returns**
```python
def test_pbo_all_nan_returns():
    """Test PBO with all NaN returns (should handle gracefully)."""
    nan_returns = np.full((10, 100), np.nan)

    result = guard_pbo(nan_returns, threshold=0.30, n_splits=4)

    # EXPECTED: Should return dict with pass=False and error message
    assert result["pass"] == False
    assert "invalid" in result.get("error", "").lower() or result["pbo"] == 0.0
```
**Why Critical**: Data quality issues (missing OHLCV) could propagate NaN values.

---

**GAP-5: Single Trial (n_trials=1)**
```python
def test_pbo_single_trial():
    """Test PBO with only one trial (should handle or raise)."""
    single_trial = np.random.randn(1, 100) * 0.01

    # EXPECTED: Either raise ValueError or return PBO=0.5 (undefined)
    # Check implementation to determine expected behavior
    result = guard_pbo(single_trial, threshold=0.30, n_splits=4)

    # If implementation allows, PBO should be 0.0 or 0.5 (no ranking possible)
    assert result["pbo"] in [0.0, 0.5] or result["pass"] == False
```
**Why Critical**: Edge case for assets with severe optimization failures.

---

#### üü° MEDIUM PRIORITY GAPS

**GAP-6: Very High PBO (>0.9)**
```python
def test_pbo_extreme_overfitting():
    """Test PBO with extreme overfitting scenario (PBO should be >0.9)."""
    np.random.seed(42)

    # Create pathological case: Best IS trial is worst OOS
    # Trial 0: Great IS (high returns), terrible OOS (negative returns)
    returns = np.random.randn(20, 200) * 0.02
    returns[0, :100] = 0.05  # First half (IS): Amazing
    returns[0, 100:] = -0.05  # Second half (OOS): Disaster

    result = guard_pbo(returns, threshold=0.30, n_splits=4)

    # EXPECTED: PBO should be very high (>0.7)
    assert result["pbo"] > 0.5, "Extreme overfitting not detected"
    assert result["pass"] == False
    assert "HIGH RISK" in result["interpretation"] or "CRITICAL" in result["interpretation"]
```
**Why Useful**: Validates PBO correctly identifies severe overfitting.

---

**GAP-7: Threshold Edge Cases**
```python
def test_pbo_threshold_edge_cases():
    """Test PBO with threshold exactly at boundary."""
    np.random.seed(42)
    returns = np.random.randn(10, 100) * 0.01

    # Test threshold = 0.0 (strictest)
    result_strict = guard_pbo(returns, threshold=0.0, n_splits=4)
    assert result_strict["threshold"] == 0.0

    # Test threshold = 1.0 (most permissive)
    result_permissive = guard_pbo(returns, threshold=1.0, n_splits=4)
    assert result_permissive["threshold"] == 1.0
    assert result_permissive["pass"] == True  # Any PBO passes

    # Test threshold > 1.0 (invalid?)
    # Check if implementation validates threshold range
```
**Why Useful**: Ensures threshold validation works correctly.

---

**GAP-8: Large n_splits (Combinatorial Explosion)**
```python
def test_pbo_large_splits_performance():
    """Test PBO with large n_splits (warns about slow execution)."""
    returns = np.random.randn(10, 1000) * 0.01

    # n_splits=20 ‚Üí C(20,10) = 184,756 combinations (very slow)
    # Should complete but may take time
    import time
    start = time.time()
    result = guard_pbo(returns, threshold=0.30, n_splits=20)
    duration = time.time() - start

    # EXPECTED: Should complete in <10 seconds
    assert duration < 10, f"PBO too slow: {duration:.1f}s"
    assert result["n_combinations"] == 184756
```
**Why Useful**: Prevents production slowdowns from excessive n_splits.

---

### Recommendation: Priority Tests to Add

**IMMEDIATE (Before Production)**:
- GAP-1 (Empty matrix) ‚Üê BLOCKER
- GAP-2 (Zero/negative splits) ‚Üê BLOCKER
- GAP-3 (Insufficient periods) ‚Üê BLOCKER

**SOON (Next Sprint)**:
- GAP-4 (All NaN) ‚Üê Data quality safeguard
- GAP-5 (Single trial) ‚Üê Edge case handling

**NICE-TO-HAVE**:
- GAP-6 (Extreme overfitting validation)
- GAP-7 (Threshold edge cases)
- GAP-8 (Performance check)

---

## S2: CPCV Tests Status

### Existing Tests
**File**: ‚ùå DOES NOT EXIST
**Expected Path**: `c:\Users\Arthur\friendly-fishstick\tests\validation\test_cpcv.py`

### Implementation Status
**CPCV Code**: ‚úÖ EXISTS at `crypto_backtest\validation\cpcv.py`
**Features**:
- `CombinatorialPurgedKFold` class
- `validate_with_cpcv()` function
- Purging and embargo logic
- Combinatorial split generation

### Proposed Test Suite (8 Tests)

#### Test 1: Basic Functionality
```python
def test_cpcv_basic_split():
    """Test CPCV generates correct number of splits."""
    from crypto_backtest.validation.cpcv import CombinatorialPurgedKFold
    import numpy as np

    cpcv = CombinatorialPurgedKFold(n_splits=6, n_test_splits=2)

    # C(6,2) = 15 combinations
    assert cpcv.get_n_splits() == 15

    # Test with dummy data
    data = np.random.randn(1000, 5)
    splits = list(cpcv.split(data))

    assert len(splits) == 15
    for train_idx, test_idx in splits:
        assert len(train_idx) > 0
        assert len(test_idx) > 0
        assert len(set(train_idx) & set(test_idx)) == 0  # No overlap
```

---

#### Test 2: Purging Logic
```python
def test_cpcv_purging():
    """Test purging removes observations near test boundaries."""
    from crypto_backtest.validation.cpcv import CombinatorialPurgedKFold
    import numpy as np

    cpcv = CombinatorialPurgedKFold(n_splits=4, n_test_splits=1, purge_gap=5)

    data = np.random.randn(400, 5)  # 100 obs per split
    splits = list(cpcv.split(data))

    for train_idx, test_idx in splits:
        # Check no train index is within purge_gap of any test index
        for test_i in test_idx:
            distances = np.abs(train_idx - test_i)
            assert np.all(distances > 5), "Purging failed - train too close to test"
```

---

#### Test 3: Embargo Logic
```python
def test_cpcv_embargo():
    """Test embargo creates gap after test set."""
    from crypto_backtest.validation.cpcv import CombinatorialPurgedKFold
    import numpy as np

    cpcv = CombinatorialPurgedKFold(n_splits=4, n_test_splits=1, embargo_pct=0.05)

    data = np.random.randn(1000, 5)
    embargo_size = int(1000 * 0.05)  # 50 observations

    splits = list(cpcv.split(data))

    for train_idx, test_idx in splits:
        test_end = max(test_idx)
        # Check no train index in embargo zone [test_end, test_end + embargo_size]
        embargo_zone = range(test_end + 1, min(test_end + embargo_size + 1, len(data)))
        assert len(set(train_idx) & set(embargo_zone)) == 0, "Embargo violated"
```

---

#### Test 4: Parameter Validation
```python
def test_cpcv_invalid_params():
    """Test CPCV raises errors for invalid parameters."""
    from crypto_backtest.validation.cpcv import CombinatorialPurgedKFold
    import pytest

    # n_test_splits >= n_splits
    with pytest.raises(ValueError, match="n_test_splits must be less than n_splits"):
        CombinatorialPurgedKFold(n_splits=4, n_test_splits=4)

    # n_splits < 2
    with pytest.raises(ValueError, match="n_splits must be at least 2"):
        CombinatorialPurgedKFold(n_splits=1, n_test_splits=1)
```

---

#### Test 5: Split Sizes
```python
def test_cpcv_split_sizes():
    """Test train/test split sizes are approximately correct."""
    from crypto_backtest.validation.cpcv import CombinatorialPurgedKFold
    import numpy as np

    cpcv = CombinatorialPurgedKFold(n_splits=6, n_test_splits=2, purge_gap=0, embargo_pct=0)

    data = np.random.randn(600, 5)  # 100 obs per split
    splits = list(cpcv.split(data))

    for train_idx, test_idx in splits:
        # Test set: 2/6 splits = ~200 observations
        assert 180 <= len(test_idx) <= 220, f"Test size wrong: {len(test_idx)}"

        # Train set: 4/6 splits = ~400 observations
        assert 360 <= len(train_idx) <= 440, f"Train size wrong: {len(train_idx)}"
```

---

#### Test 6: get_all_splits() Method
```python
def test_cpcv_get_all_splits():
    """Test get_all_splits returns CPCVSplit objects."""
    from crypto_backtest.validation.cpcv import CombinatorialPurgedKFold
    import numpy as np

    cpcv = CombinatorialPurgedKFold(n_splits=4, n_test_splits=1)

    data = np.random.randn(400, 5)
    splits = cpcv.get_all_splits(data)

    # C(4,1) = 4 combinations
    assert len(splits) == 4

    for i, split in enumerate(splits):
        assert split.split_id == i
        assert len(split.train_indices) > 0
        assert len(split.test_indices) > 0
        assert isinstance(split.train_indices, np.ndarray)
        assert isinstance(split.test_indices, np.ndarray)
```

---

#### Test 7: validate_with_cpcv() Function
```python
def test_validate_with_cpcv():
    """Test validate_with_cpcv aggregates results correctly."""
    from crypto_backtest.validation.cpcv import validate_with_cpcv
    import pandas as pd

    # Create dummy data
    dates = pd.date_range("2020-01-01", periods=1000, freq="1h")
    data = pd.DataFrame({
        "open": 100 + np.random.randn(1000),
        "high": 101 + np.random.randn(1000),
        "low": 99 + np.random.randn(1000),
        "close": 100 + np.random.randn(1000),
        "volume": 1000 + np.random.randn(1000) * 100,
    }, index=dates)

    # Dummy strategy function
    def dummy_strategy(df):
        return {"sharpe_ratio": 1.0, "total_return": 0.05}

    result = validate_with_cpcv(
        data,
        dummy_strategy,
        n_splits=4,
        n_test_splits=1,
        purge_gap=2,
        embargo_pct=0.01,
    )

    # Check result format
    assert "n_combinations" in result
    assert "mean_is_sharpe" in result
    assert "mean_oos_sharpe" in result
    assert "wfe_mean" in result
    assert "all_results" in result

    # Check WFE calculation
    expected_wfe = result["mean_oos_sharpe"] / result["mean_is_sharpe"]
    assert abs(result["wfe_mean"] - expected_wfe) < 0.01
```

---

#### Test 8: Edge Case - Tiny Dataset
```python
def test_cpcv_tiny_dataset():
    """Test CPCV handles very small datasets gracefully."""
    from crypto_backtest.validation.cpcv import CombinatorialPurgedKFold
    import numpy as np

    cpcv = CombinatorialPurgedKFold(n_splits=2, n_test_splits=1)

    # Only 20 observations ‚Üí 10 per split
    tiny_data = np.random.randn(20, 5)
    splits = list(cpcv.split(tiny_data))

    assert len(splits) == 2  # C(2,1) = 2

    for train_idx, test_idx in splits:
        # Should still split correctly
        assert len(train_idx) + len(test_idx) <= 20
```

---

### Recommendation: CPCV Test Priority

**Status**: ‚ö†Ô∏è OPTIONAL (CPCV is on-demand only, not blocking production)

**If time permits**:
- Add tests 1-4 (Basic + Validation) ‚Üê Core functionality
- Add test 7 (validate_with_cpcv) ‚Üê Integration check
- Tests 5-6, 8 are nice-to-have

**Effort**: M (Medium) - 2-3 hours to write all 8 tests

---

## S3: Validation Protocol

**File Created**: `c:\Users\Arthur\friendly-fishstick\docs\SAM_VALIDATION_PROTOCOL.md`

### Sections Included:
1. ‚úÖ **Phase 1: Unit Tests** - Pytest commands, expected results, failure actions
2. ‚úÖ **Phase 2: Integration Tests** - Single asset (ETH), multi-asset (3), expected WFE drops
3. ‚úÖ **Phase 3: Regression Check** - All 14 PROD assets, PASS/FAIL thresholds
4. ‚úÖ **Phase 4: Manual Verification** - Visual inspection, spot checks, PBO error validation

### Key Features:
- **Exact Commands**: Copy-paste ready pytest and python commands
- **Expected Outputs**: Specific values for WFE (e.g., ETH: 1.22 ‚Üí 0.65-0.85)
- **PASS/FAIL Criteria**: Clear thresholds (e.g., ‚â§2 new FAILs = acceptable)
- **Rollback Procedure**: Git commands, root cause analysis steps
- **Duration Estimates**: 2.5-3.5 hours total, parallelizable

### Critical Validation Points:
1. **WFE Normalization**: All suspect assets (WFE >1.0) must drop to 0.5-0.9
2. **No Regressions**: Max 2 new FAILs tolerated in 14 PROD assets
3. **PBO Graceful Failure**: Must not crash pipeline when returns_matrix unavailable
4. **Formula Verification**: Manual spot check of `wfe_pardo = oos_sharpe / is_sharpe`

---

## S4: PBO Activation Gap Analysis

### Current Blocker: `returns_matrix` Not Tracked

**Location**: Line 1002 in `scripts\run_guards_multiasset.py`
```python
returns_matrix=None,  # TODO: Track per-trial returns for PBO
```

**Root Cause**: Optimization pipeline does not store per-trial returns, only final best params.

---

### What Needs to Happen

#### Step 1: Modify Optimizer to Track Returns
**File**: `crypto_backtest\optimization\parallel_optimizer.py`

**Current Flow**:
```
1. Optuna runs N trials (e.g., 300)
2. For each trial:
   - Run backtest with trial params
   - Compute Sharpe ratio
   - Optuna records objective value (Sharpe)
3. Return best params only
```

**Needed Flow**:
```
1. Optuna runs N trials (e.g., 300)
2. For each trial:
   - Run backtest with trial params
   - Compute Sharpe ratio
   - **STORE returns series in array (NEW)**
3. Return best params + **returns_matrix (NEW)**
```

---

#### Step 2: Implementation Pseudocode

**In `parallel_optimizer.py` around line 400-500 (optimize_asset function)**:

```python
def optimize_asset(...):
    # EXISTING CODE
    study = optuna.create_study(...)

    # NEW: Initialize returns storage
    returns_matrix = []  # List to store returns arrays

    # MODIFY OBJECTIVE to capture returns
    def objective_with_tracking(trial):
        # ... existing param sampling ...
        result = backtester.run(data, strategy)
        metrics = compute_metrics(result.equity_curve, result.trades)

        # NEW: Store returns for this trial
        returns_series = result.equity_curve.pct_change().dropna()
        returns_matrix.append(returns_series.values)

        return metrics["sharpe_ratio"]

    study.optimize(objective_with_tracking, n_trials=n_trials)

    # NEW: Convert to numpy array (trials √ó periods)
    returns_array = np.array(returns_matrix)  # Shape: (300, ~8000)

    # Return returns_array alongside best_params
    return {
        "best_params": study.best_params,
        "returns_matrix": returns_array,  # NEW
        ...
    }
```

---

#### Step 3: Propagate to Guards Script

**In `run_guards_multiasset.py` around line 936 (_asset_guard_worker)**:

```python
def _asset_guard_worker(...):
    # ... existing code ...

    # NEW: Get returns_matrix from optimization result (if available)
    # Option A: Load from saved optimization output
    optim_results_path = f"outputs/{asset}_optim_results.pkl"
    if Path(optim_results_path).exists():
        import pickle
        with open(optim_results_path, "rb") as f:
            optim_data = pickle.load(f)
            returns_matrix = optim_data.get("returns_matrix", None)
    else:
        returns_matrix = None

    # Pass to guards
    guard_results = _run_guards_parallel(
        ...
        returns_matrix=returns_matrix,  # Now populated
        ...
    )
```

---

#### Step 4: Storage Strategy

**Challenge**: `returns_matrix` is LARGE
- 300 trials √ó 8000 bars = 2.4M floats
- ~19 MB per asset (as float64)
- 14 assets = ~270 MB total

**Options**:

**Option A: In-Memory Only (RECOMMENDED)**
```python
# Pros: Fast, no disk I/O
# Cons: Lost after run (must rerun optimization to get PBO)
# Use case: Real-time validation during optimization
```

**Option B: Pickle to Disk**
```python
# Pros: Persistent, can compute PBO later
# Cons: Large files, slow load times
# Use case: Post-analysis, research
import pickle
with open(f"outputs/{asset}_returns_matrix.pkl", "wb") as f:
    pickle.dump(returns_matrix, f)
```

**Option C: Compressed Parquet**
```python
# Pros: Good compression (~5x), fast read
# Cons: More complex code
# Use case: Production archival
import pandas as pd
df = pd.DataFrame(returns_matrix.T)  # Transpose: periods √ó trials
df.to_parquet(f"outputs/{asset}_returns_matrix.parquet", compression="zstd")
```

**RECOMMENDATION**: Start with **Option A** (in-memory), add Option C later if needed.

---

### Effort Estimate

| Task | Effort | Risk |
|------|--------|------|
| Modify objective function to store returns | M (2-3h) | Low |
| Convert returns list to numpy array | S (30min) | Low |
| Update return signature of optimize_asset | S (30min) | Low |
| Propagate returns_matrix to guards script | M (1-2h) | Medium |
| Handle None case gracefully (already done) | ‚úÖ DONE | Low |
| Test with 1 asset (ETH) | M (1h) | Medium |
| Test with 14 assets (memory usage) | L (2h) | High |
| **TOTAL** | **L (7-9h)** | **Medium** |

**Complexity**: MEDIUM
- Code changes straightforward
- Main risk: Memory management with 14 assets in parallel
- May need to reduce workers or clear memory between assets

---

### Risk Assessment

#### Risk 1: Memory Explosion (High)
**Issue**: 270 MB √ó 6 workers = 1.6 GB RAM
**Mitigation**:
- Use `workers=1` when PBO enabled
- OR clear returns_matrix after guard execution
- OR use Option C (disk-based storage)

---

#### Risk 2: Slow Trial Execution (Medium)
**Issue**: Storing returns adds overhead per trial
**Mitigation**:
- Returns storage is O(1) per trial (just append)
- Negligible impact (<1% slowdown)

---

#### Risk 3: Breaking Existing Pipeline (Medium)
**Issue**: Changing return signature of optimize_asset breaks callers
**Mitigation**:
- Make returns_matrix optional in return dict
- Update all callers to handle new field
- Add unit test for backward compatibility

---

### Activation Checklist

**Prerequisites**:
- [ ] Jordan's WFE DUAL merged
- [ ] Guard-008 PBO code stable
- [ ] Unit tests for PBO PASS

**Implementation**:
- [ ] Modify parallel_optimizer.py objective function
- [ ] Update optimize_asset return signature
- [ ] Propagate returns_matrix to guards script
- [ ] Add storage option (pickle or parquet)
- [ ] Test with ETH (single asset)
- [ ] Test with 3 assets (memory check)

**Validation**:
- [ ] PBO computes without crash
- [ ] PBO values in [0, 1] range
- [ ] PBO interpretation correct
- [ ] Memory usage acceptable (<2 GB)

**Production**:
- [ ] Run all 14 assets with PBO
- [ ] Document PBO thresholds (current: 0.30)
- [ ] Add PBO to validation reports

**Estimated Completion**: 1-2 sprints (if prioritized after WFE DUAL)

---

## Summary of Deliverables

### Files Created
1. **`docs\SAM_VALIDATION_PROTOCOL.md`** (5.9 KB)
   - 4-phase validation protocol
   - Exact commands, expected outputs, PASS/FAIL criteria
   - Duration: 2.5-3.5 hours total

2. **`docs\SAM_DELIVERABLES.md`** (this file)
   - PBO test gaps (8 gaps identified, 3 critical)
   - CPCV test suite (8 proposed tests)
   - PBO activation gap analysis
   - Implementation roadmap

---

### S1: PBO Tests Review
- **Existing**: 8 tests, well-covered basics
- **Coverage**: Good for happy path, missing edge cases
- **Gaps**: 8 identified (3 critical, 2 medium, 3 nice-to-have)
- **Action**: Add GAP-1 to GAP-3 before production

---

### S2: CPCV Tests Status
- **Existing**: ‚ùå None
- **Coverage**: N/A (code exists, tests missing)
- **Proposed**: 8 tests (core functionality + edge cases)
- **Priority**: OPTIONAL (CPCV on-demand only)
- **Effort**: M (2-3 hours)

---

### S3: Validation Protocol
- **Status**: ‚úÖ COMPLETE
- **File**: `docs\SAM_VALIDATION_PROTOCOL.md`
- **Sections**: 4 phases (unit ‚Üí integration ‚Üí regression ‚Üí manual)
- **Key Validation**: WFE normalization (>1.0 ‚Üí 0.5-0.9)

---

### S4: PBO Activation Gap
- **Blocker**: `returns_matrix` not tracked in optimization pipeline
- **Implementation**: Modify objective function to store per-trial returns
- **Effort**: L (7-9 hours)
- **Risk**: Medium (memory management with parallel workers)
- **Storage**: In-memory (recommended) or disk-based (optional)
- **Timeline**: 1-2 sprints post-WFE DUAL

---

## Next Actions

### Immediate (Sam)
1. ‚úÖ Review protocol with Jordan/Casey
2. ‚è∏Ô∏è Wait for Jordan's WFE DUAL + PBO merge
3. Execute Phase 1 validation (unit tests)

### Short-Term (Jordan + Sam)
1. Add critical PBO tests (GAP-1 to GAP-3)
2. Execute Phase 2-4 validation (integration + regression)
3. Decide on CPCV tests priority

### Medium-Term (Alex + Jordan)
1. Implement returns_matrix tracking in optimizer
2. Activate PBO in production pipeline
3. Validate PBO on all 14 assets

---

**END OF DELIVERABLES**
