"""
Experiment: Meme Coin Cluster Validation

Alex R&D Plan - Track 2: EXPAND (2.5 Meme Coin Cluster Validation)

Based on SHIB Investigation findings:
- SHIB Sharpe 5.67, WFE 2.27 â€” genuine edge confirmed
- Pattern: High trade frequency, SIDEWAYS dominance, low sensitivity

This script validates if PEPE, BONK, WIF show the same pattern.

Expected characteristics:
- High trade frequency (400+ trades)
- SIDEWAYS dominance (>50% of PnL)
- WFE > 1.0 (OOS better than IS)
- Low sensitivity variance (<10%)

Usage:
    # Download data first if needed
    python scripts/download_data.py --assets PEPE BONK WIF
    
    # Then run validation
    python scripts/experiment_meme_cluster.py
    python scripts/experiment_meme_cluster.py --assets PEPE BONK WIF FLOKI
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import argparse
import json
from datetime import datetime
from typing import List, Dict, Any

import pandas as pd
import numpy as np

from crypto_backtest.optimization.parallel_optimizer import (
    load_data,
    split_data,
    optimize_atr,
    optimize_ichimoku,
    build_strategy_params,
    run_backtest,
    monte_carlo_pvalue,
    OPTIM_CONFIG,
)
from crypto_backtest.engine.backtest import BacktestConfig, VectorizedBacktester
from crypto_backtest.strategies.final_trigger import FinalTriggerStrategy
from crypto_backtest.analysis.metrics import compute_metrics
from crypto_backtest.analysis.regime import classify_regimes_v2
from crypto_backtest.config.scan_assets import (
    MEME_COINS,
    MEME_COIN_DISPLACEMENT,
    ATR_SEARCH_SPACE_HIGH_VOL,
)


BASE_CONFIG = BacktestConfig(
    initial_capital=10000.0,
    fees_bps=5.0,
    slippage_bps=2.0,
    sizing_mode="fixed",
    intrabar_order="stop_first",
)


def analyze_regime_performance(
    data: pd.DataFrame,
    trades: pd.DataFrame,
) -> Dict[str, Any]:
    """Analyze performance by regime."""
    if trades.empty:
        return {}
    
    regimes = classify_regimes_v2(data)
    
    regime_stats = {}
    total_pnl = trades["pnl"].sum() if "pnl" in trades.columns else 0
    
    for regime in ["BULL", "BEAR", "SIDEWAYS", "RECOVERY", "HIGH_VOL", "CRASH", "OTHER"]:
        # Find trades that entered during this regime
        regime_trades = []
        for _, trade in trades.iterrows():
            entry_time = trade.get("entry_time")
            if entry_time is not None and entry_time in regimes.index:
                if regimes.loc[entry_time] == regime:
                    regime_trades.append(trade)
        
        if not regime_trades:
            regime_stats[regime] = {"n_trades": 0, "pnl": 0, "pnl_pct": 0}
            continue
        
        regime_df = pd.DataFrame(regime_trades)
        regime_pnl = regime_df["pnl"].sum() if "pnl" in regime_df.columns else 0
        
        regime_stats[regime] = {
            "n_trades": len(regime_df),
            "pnl": regime_pnl,
            "pnl_pct": (regime_pnl / abs(total_pnl) * 100) if total_pnl != 0 else 0,
        }
    
    return regime_stats


def run_sensitivity_test(
    data: pd.DataFrame,
    params: dict,
    perturbation: float = 0.1,
    n_tests: int = 20,
) -> float:
    """Run parameter sensitivity test."""
    
    base_result = run_backtest(data, params)
    base_sharpe = base_result["sharpe"]
    
    sharpe_values = [base_sharpe]
    
    # Perturb each numeric parameter
    for _ in range(n_tests):
        perturbed_params = params.copy()
        
        # Perturb ATR multipliers
        for key in ["sl_mult", "tp1_mult", "tp2_mult", "tp3_mult"]:
            if key in perturbed_params:
                delta = np.random.uniform(-perturbation, perturbation)
                perturbed_params[key] = perturbed_params[key] * (1 + delta)
        
        # Perturb Ichimoku params
        if "ichimoku" in perturbed_params:
            for key in ["tenkan", "kijun"]:
                if key in perturbed_params["ichimoku"]:
                    delta = np.random.randint(-2, 3)
                    perturbed_params["ichimoku"][key] = max(
                        5, perturbed_params["ichimoku"][key] + delta
                    )
        
        try:
            result = run_backtest(data, perturbed_params)
            sharpe_values.append(result["sharpe"])
        except:
            pass
    
    # Calculate coefficient of variation (std / mean)
    if len(sharpe_values) > 1 and np.mean(sharpe_values) != 0:
        cv = np.std(sharpe_values) / abs(np.mean(sharpe_values)) * 100
    else:
        cv = 100  # High variance if can't compute
    
    return cv


def validate_meme_coin(
    asset: str,
    data_dir: str = "data",
    n_trials: int = 300,
    displacement: int = None,
) -> Dict[str, Any]:
    """
    Run full validation on a meme coin asset.
    
    Checks if asset exhibits the meme coin pattern:
    - High trade frequency
    - SIDEWAYS regime dominance
    - WFE > 1.0
    - Low sensitivity variance
    """
    print(f"\n{'='*60}")
    print(f"VALIDATING MEME COIN: {asset}")
    print("=" * 60)
    
    if displacement is None:
        displacement = MEME_COIN_DISPLACEMENT
    
    try:
        # Load data
        df = load_data(asset, data_dir)
        df = df.iloc[OPTIM_CONFIG["warmup_bars"]:]
        df_is, df_val, df_oos = split_data(df, splits=(0.6, 0.2, 0.2))
        
        print(f"Data: IS={len(df_is)}, VAL={len(df_val)}, OOS={len(df_oos)} bars")
        
        # Optimize with HIGH_VOL ranges (appropriate for meme coins)
        print(f"Optimizing with displacement={displacement}, HIGH_VOL ranges...")
        
        atr_params, atr_sharpe = optimize_atr(
            df_is,
            n_trials=n_trials,
            min_trades=50,
            enforce_tp_progression=True,
            fixed_displacement=displacement,
            search_space=ATR_SEARCH_SPACE_HIGH_VOL,
        )
        
        ichi_params, ichi_sharpe = optimize_ichimoku(
            df_is,
            atr_params,
            n_trials=n_trials,
            min_trades=50,
            fixed_displacement=displacement,
        )
        
        # Build final params
        final_params = build_strategy_params(
            sl_mult=atr_params["sl_mult"],
            tp1_mult=atr_params["tp1_mult"],
            tp2_mult=atr_params["tp2_mult"],
            tp3_mult=atr_params["tp3_mult"],
            tenkan=ichi_params["tenkan"],
            kijun=ichi_params["kijun"],
            tenkan_5=ichi_params["tenkan_5"],
            kijun_5=ichi_params["kijun_5"],
            displacement=displacement,
        )
        
        # Evaluate
        is_result = run_backtest(df_is, final_params)
        oos_result = run_backtest(df_oos, final_params)
        
        # Run full backtest to get trades
        from crypto_backtest.optimization.bayesian import _instantiate_strategy
        strategy = _instantiate_strategy(FinalTriggerStrategy, final_params)
        backtester = VectorizedBacktester(BASE_CONFIG)
        full_result = backtester.run(df_oos, strategy)
        trades = full_result.trades
        
        # Calculate metrics
        wfe = oos_result["sharpe"] / is_result["sharpe"] if is_result["sharpe"] > 0 else 0
        
        # Regime analysis
        print("Analyzing regime performance...")
        regime_stats = analyze_regime_performance(df_oos, trades)
        sideways_pct = regime_stats.get("SIDEWAYS", {}).get("pnl_pct", 0)
        recovery_pct = regime_stats.get("RECOVERY", {}).get("pnl_pct", 0)
        
        # Sensitivity test
        print("Running sensitivity test...")
        sensitivity_cv = run_sensitivity_test(df_oos, final_params)
        
        # Monte Carlo
        print("Running Monte Carlo test...")
        mc_p = monte_carlo_pvalue(df_oos, final_params, oos_result["sharpe"], iterations=500)
        
        # Check meme coin criteria
        is_high_trade_count = oos_result["trades"] >= 100  # Relaxed from 400
        is_sideways_dominant = sideways_pct > 40  # >40% of PnL from SIDEWAYS
        is_wfe_strong = wfe > 0.8  # WFE > 0.8 (relaxed from 1.0)
        is_low_sensitivity = sensitivity_cv < 15  # CV < 15%
        
        passes_criteria = (
            is_high_trade_count and
            is_sideways_dominant and
            is_wfe_strong and
            is_low_sensitivity
        )
        
        result = {
            "asset": asset,
            "status": "PASS" if passes_criteria else "FAIL",
            "displacement": displacement,
            "params": {
                "sl_mult": atr_params["sl_mult"],
                "tp1_mult": atr_params["tp1_mult"],
                "tp2_mult": atr_params["tp2_mult"],
                "tp3_mult": atr_params["tp3_mult"],
                "tenkan": ichi_params["tenkan"],
                "kijun": ichi_params["kijun"],
            },
            "metrics": {
                "is_sharpe": is_result["sharpe"],
                "oos_sharpe": oos_result["sharpe"],
                "wfe": wfe,
                "oos_trades": oos_result["trades"],
                "mc_p_value": mc_p,
                "sensitivity_cv": sensitivity_cv,
            },
            "regime_analysis": regime_stats,
            "criteria_check": {
                "high_trade_count": is_high_trade_count,
                "sideways_dominant": is_sideways_dominant,
                "wfe_strong": is_wfe_strong,
                "low_sensitivity": is_low_sensitivity,
            },
        }
        
        # Print summary
        print(f"\n--- Results for {asset} ---")
        print(f"IS Sharpe: {is_result['sharpe']:.2f}")
        print(f"OOS Sharpe: {oos_result['sharpe']:.2f}")
        print(f"WFE: {wfe:.2f}")
        print(f"OOS Trades: {oos_result['trades']}")
        print(f"SIDEWAYS PnL %: {sideways_pct:.1f}%")
        print(f"RECOVERY PnL %: {recovery_pct:.1f}%")
        print(f"Sensitivity CV: {sensitivity_cv:.1f}%")
        print(f"MC p-value: {mc_p:.3f}")
        print(f"MEME COIN PATTERN: {'CONFIRMED' if passes_criteria else 'NOT CONFIRMED'}")
        
        return result
        
    except Exception as e:
        print(f"ERROR: {e}")
        return {
            "asset": asset,
            "status": "ERROR",
            "error": str(e),
        }


def main():
    parser = argparse.ArgumentParser(description="Meme Coin Cluster Validation")
    parser.add_argument(
        "--assets",
        nargs="+",
        default=["PEPE", "BONK", "WIF"],
        help="Meme coins to validate"
    )
    parser.add_argument("--trials", type=int, default=300, help="Trials per optimization")
    parser.add_argument("--displacement", type=int, default=26, help="Fixed displacement")
    parser.add_argument("--data-dir", type=str, default="data", help="Data directory")
    args = parser.parse_args()
    
    print("=" * 70)
    print("MEME COIN CLUSTER VALIDATION")
    print("=" * 70)
    print(f"Assets: {args.assets}")
    print(f"Displacement: {args.displacement}")
    print(f"Trials: {args.trials}")
    
    all_results = []
    
    for asset in args.assets:
        result = validate_meme_coin(
            asset=asset,
            data_dir=args.data_dir,
            n_trials=args.trials,
            displacement=args.displacement,
        )
        all_results.append(result)
    
    # Summary
    print("\n" + "=" * 70)
    print("VALIDATION SUMMARY")
    print("=" * 70)
    
    passed = [r for r in all_results if r.get("status") == "PASS"]
    failed = [r for r in all_results if r.get("status") == "FAIL"]
    errors = [r for r in all_results if r.get("status") == "ERROR"]
    
    print(f"\nPASSED ({len(passed)}):")
    for r in passed:
        m = r.get("metrics", {})
        print(f"  {r['asset']}: Sharpe={m.get('oos_sharpe', 0):.2f}, WFE={m.get('wfe', 0):.2f}")
    
    print(f"\nFAILED ({len(failed)}):")
    for r in failed:
        cc = r.get("criteria_check", {})
        reasons = [k for k, v in cc.items() if not v]
        print(f"  {r['asset']}: Failed criteria: {', '.join(reasons)}")
    
    if errors:
        print(f"\nERRORS ({len(errors)}):")
        for r in errors:
            print(f"  {r['asset']}: {r.get('error', 'Unknown error')}")
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    Path("outputs").mkdir(exist_ok=True)
    
    output_path = f"outputs/meme_cluster_validation_{timestamp}.json"
    with open(output_path, "w") as f:
        json.dump(all_results, f, indent=2, default=str)
    
    print(f"\nDetailed results saved to: {output_path}")
    
    # Create summary CSV
    summary_rows = []
    for r in all_results:
        m = r.get("metrics", {})
        cc = r.get("criteria_check", {})
        summary_rows.append({
            "asset": r["asset"],
            "status": r.get("status", "ERROR"),
            "oos_sharpe": m.get("oos_sharpe", 0),
            "wfe": m.get("wfe", 0),
            "oos_trades": m.get("oos_trades", 0),
            "sensitivity_cv": m.get("sensitivity_cv", 0),
            "mc_p_value": m.get("mc_p_value", 1),
            "high_trade_count": cc.get("high_trade_count", False),
            "sideways_dominant": cc.get("sideways_dominant", False),
            "wfe_strong": cc.get("wfe_strong", False),
            "low_sensitivity": cc.get("low_sensitivity", False),
        })
    
    summary_df = pd.DataFrame(summary_rows)
    summary_path = f"outputs/meme_cluster_summary_{timestamp}.csv"
    summary_df.to_csv(summary_path, index=False)
    print(f"Summary CSV saved to: {summary_path}")


if __name__ == "__main__":
    main()
